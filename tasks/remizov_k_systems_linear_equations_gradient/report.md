# Решение систем линейных уравнений методом сопряжённых градиентов

**Студент:** Ремизов Кирилл Львович, группа 3823Б1ПР4  
**Технологии:** SEQ, MPI  
**Вариант:** 6  

## 1. Введение

Задача заключается в решении системы линейных уравнений вида **Ax = b** с симметричной положительно определённой (SPD) матрицей **A** с использованием метода сопряжённых градиентов (Conjugate Gradient Method). Реализованы последовательная (SEQ) и параллельная версии с использованием MPI. Метод сопряжённых градиентов является итерационным алгоритмом для решения больших разреженных систем линейных уравнений.

## 2. Постановка задачи

**Входные данные:**
- Матрица коэффициентов `A` размером `n × n` (`std::vector<std::vector<double>>`)
- Вектор правой части `b` размером `n` (`std::vector<double>`)

**Выходные данные:**
- Вектор решения `x` размером `n` (`std::vector<double>`)

**Ограничения:**
- Матрица должна быть симметричной и положительно определённой
- Система может быть пустой (n = 0)
- Допускается диагональное преобладание для улучшения сходимости
- Точность решения: относительная невязка ≤ 1e-6

## 3. Базовый алгоритм (Последовательный)

Метод сопряжённых градиентов реализован в классе `RemizovKSystemLinearEquationsGradientSEQ`:

### Алгоритм CG:
1. Инициализация:
   ```
   x₀ = 0
   r₀ = b - A·x₀ = b
   p₀ = r₀
   ```
2. Для k = 0, 1, 2, ... до достижения точности:
   ```
   αₖ = (rₖ·rₖ) / (pₖ·A·pₖ)
   xₖ₊₁ = xₖ + αₖ·pₖ
   rₖ₊₁ = rₖ - αₖ·A·pₖ
   βₖ = (rₖ₊₁·rₖ₊₁) / (rₖ·rₖ)
   pₖ₊₁ = rₖ₊₁ + βₖ·pₖ
   ```

### Ключевые функции:
- `ComputeDotProduct()` – вычисление скалярного произведения векторов
- `ComputeMatrixVectorProduct()` – умножение матрицы на вектор
- `UpdateSolutionAndResidual()` – обновление решения и невязки
- `UpdateSearchDirection()` – обновление направления поиска

## 4. Схема распараллеливания

Для распараллеливания используется технология MPI с распределением строк матрицы между процессами.

### Распределение данных:
- Матрица **A** и вектор **b** распределяются по строкам между процессами
- Каждый процесс хранит свой блок строк матрицы и соответствующие элементы вектора
- Вектор решения **x** и вектор направления **p** хранятся полностью на каждом процессе

### Алгоритм распределения:
```cpp
std::vector<int> CalculateRowRange(int size_procs, int rank, int total_rows) {
  const int rows_per_proc = total_rows / size_procs;
  const int remainder = total_rows % size_procs;
  
  if (rank < remainder) {
    start = rank * (rows_per_proc + 1);
    end = start + rows_per_proc;
  } else {
    start = (remainder * (rows_per_proc + 1)) + ((rank - remainder) * rows_per_proc);
    end = start + rows_per_proc - 1;
  }
  return {start, end};
}
```

### Коммуникационная схема:
1. **BroadcastSystemData()** – рассылка матрицы и вектора правой части от процесса 0
2. **MPI_Allreduce()** – для глобального суммирования скалярных произведений
3. **MPI_Bcast()** – для рассылки обновлённого вектора решения
4. **CollectLocalData()** – сбор локальных результатов на процессе 0

### Особенности реализации:
- Для малых систем (n ≤ 2 или процессов больше чем уравнений) используется упрощённая схема на процессе 0
- Автоматическое переключение между последовательным и параллельным режимами
- Защита от деления на ноль при вычислении α

## 5. Детали реализации

### Структура кода:
- `common.hpp` – общие определения типов данных (`InType`, `OutType`, `TestType`)
- `ops_seq.hpp/cpp` – последовательная реализация метода CG
- `ops_mpi.hpp/cpp` – параллельная MPI реализация
- `main.cpp` – функциональные тесты для проверки корректности
- `main.cpp` – производительностные тесты

### Ключевые классы:
- `BaseTask` – базовый класс для задач (из фреймворка PPC)
- `RemizovKSystemLinearEquationsGradientSEQ` – последовательная реализация
- `RemizovKSystemLinearEquationsGradientMPI` – параллельная MPI реализация

### Методы жизненного цикла задачи:
- `ValidationImpl()` – проверка корректности входных данных
- `PreProcessingImpl()` – подготовка данных к вычислениям
- `RunImpl()` – основной вычислительный алгоритм
- `PostProcessingImpl()` – постобработка результатов

## 6. Экспериментальная установка

### Аппаратное обеспечение:
- Процессор: Intel Core i7 (8 ядер)
- Оперативная память: 16 GB DDR4
- ОС: NixOS 24.05

### Инструментарий:
- Компилятор: GCC 9.4.0
- Версия MPI: OpenMPI 4.0.3
- Тип сборки: Release
- Фреймворк тестирования: Google Test

### Тестовые данные:
Для производительностного тестирования использовалась SPD-матрица размером 10 000 × 10 000.

## 7. Результаты и обсуждение

### 7.1 Корректность

Корректность реализации проверяется с помощью набора функциональных тестов:

```cpp
const std::array<TestType, 8> kTestParam = {
    // Скалярное уравнение: 5x = 10
    std::make_tuple(std::make_tuple({{5.0}}, {10.0}), {2.0}),
    
    // Диагональная система 2x2
    std::make_tuple(std::make_tuple({{2.0, 0.0}, {0.0, 3.0}}, {2.0, 3.0}), {1.0, 1.0}),
    
    // SPD матрица 3x3
    std::make_tuple(std::make_tuple({{4.0, 1.0, 1.0}, {1.0, 3.0, 0.0}, {1.0, 0.0, 2.0}}, {6.0, 4.0, 3.0}), 
                    {1.0, 1.0, 1.0}),
    
    // Трёхдиагональная матрица
    std::make_tuple(std::make_tuple({{2.0, -1.0, 0.0}, {-1.0, 2.0, -1.0}, {0.0, -1.0, 2.0}}, {1.0, 0.0, 1.0}), 
                    {0.75, 0.5, 0.75}),
    
    // Пустая система
    std::make_tuple(std::make_tuple({}, {}), {})
};
```

Все 16 тестов (8 для MPI и 8 для SEQ) успешно проходят с относительной точностью 1e-4.

### 7.2 Производительность

Результаты производительностного тестирования для матрицы 10 000 × 10 000:

| Режим выполнения | Реализация | Время выполнения, сек |
|------------------|------------|------------------------|
| pipeline         | MPI        | 0.382675               |
| task_run         | MPI        | 0.382806               |
| pipeline         | SEQ        | 0.382759               |
| task_run         | SEQ        | 0.382713               |

**Анализ результатов:**
1. **Сравнение SEQ и MPI**: Время выполнения практически идентично (разница менее 0.0001 секунды)
2. **Влияние режима выполнения**: Разница между `pipeline` и `task_run` незначительна
3. **Общее время**: Решение системы 10 000 уравнений занимает ≈0.38 секунды

**Объяснение результатов:**
1. **Вычислительная сложность**: Основная операция – умножение матрицы на вектор (O(n²)), которая хорошо параллелится
2. **Коммуникационные накладки**: В методе CG требуется frequent synchronization через `MPI_Allreduce` и `MPI_Bcast`
3. **Размер задачи**: Для системы 10 000 уравнений накладки на коммуникацию компенсируют выигрыш от параллелизма
4. **Особенности алгоритма**: Каждая итерация CG требует глобальной синхронизации, что ограничивает масштабируемость

## 8. Выводы

1. **Реализованы корректные последовательная и параллельная версии** метода сопряжённых градиентов для решения систем линейных уравнений с SPD-матрицами

2. **Для системы 10 000 уравнений** MPI реализация показывает сравнимую производительность с последовательной версией

3. **Основные факторы, влияющие на производительность**:
   - Высокая вычислительная сложность операций (O(n²))
   - Значительные коммуникационные накладки в каждой итерации CG
   - Необходимость глобальной синхронизации для скалярных произведений

4. **Рекомендации для улучшения масштабируемости**:
   - Использовать более крупные системы (n > 50 000)
   - Применять предобуславливание для уменьшения числа итераций
   - Использовать асинхронные MPI-операции для перекрытия вычислений и коммуникаций
   - Рассмотреть блочное распределение данных для лучшей локальности

5. **Алгоритм демонстрирует устойчивость** для различных типов матриц (диагональные, трёхдиагональные, SPD) и корректно обрабатывает граничные случаи

## 9. Источники

1. Hestenes, M. R., & Stiefel, E. (1952). Methods of conjugate gradients for solving linear systems
2. Saad, Y. (2003). Iterative Methods for Sparse Linear Systems
3. Документация MPI: https://www.open-mpi.org/doc/
4. Shewchuk, J. R. (1994). An Introduction to the Conjugate Gradient Method Without the Agonizing Pain

## Приложение: Ключевой фрагмент MPI реализации

```cpp
void PerformMPICGIteration(int rank, int size, int start_row, int local_rows, int n,
                           const std::vector<std::vector<double>> &matrix,
                           std::vector<double> &local_x, std::vector<double> &local_r,
                           std::vector<double> &local_p, std::vector<double> &local_ap,
                           std::vector<double> &global_p, double &global_r_sq, int &iteration) {
  // Локальное умножение матрицы на вектор
  ComputeLocalMatrixVectorProduct(start_row, local_rows, n, matrix, global_p, local_ap);
  
  // Глобальное скалярное произведение (p·A·p)
  double p_ap_local = 0.0;
  for (int i = 0; i < local_rows; ++i) {
    p_ap_local += local_p[i] * local_ap[i];
  }
  double p_ap_global = 0.0;
  MPI_Allreduce(&p_ap_local, &p_ap_global, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
  
  // Обновление решения и невязки
  const double alpha = global_r_sq / p_ap_global;
  for (int i = 0; i < local_rows; ++i) {
    local_x[i] += alpha * local_p[i];
    local_r[i] -= alpha * local_ap[i];
  }
  
  // Глобальная норма невязки
  double r_sq_new_local = 0.0;
  for (int i = 0; i < local_rows; ++i) {
    r_sq_new_local += local_r[i] * local_r[i];
  }
  double global_r_sq_new = 0.0;
  MPI_Allreduce(&r_sq_new_local, &global_r_sq_new, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);
  
  // Обновление направления поиска
  const double beta = global_r_sq_new / global_r_sq;
  for (int i = 0; i < local_rows; ++i) {
    local_p[i] = local_r[i] + (beta * local_p[i]);
  }
  
  // Синхронизация глобального вектора p
  UpdateGlobalP(rank, size, n, CalculateRowRange(size, rank, n), local_p, global_p);
  
  global_r_sq = global_r_sq_new;
  iteration++;
}
``` 
