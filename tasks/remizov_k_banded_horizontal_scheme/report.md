# Умножение матриц с использованием ленточной горизонтальной схемы

- **Студент**: Ремизов Кирилл Львович, группа 3823Б1ПР4  
- **Технологии**: SEQ (последовательная), MPI (распределённая)  
- **Вариант**: 13 (умножение матриц)

## 1. Введение

Задача состоит в реализации умножения двух матриц. Были разработаны последовательная (SEQ) и параллельная (MPI) версии алгоритма. Параллельная реализация использует горизонтальную схему распределения строк первой матрицы между процессами для ускорения вычислений на многопроцессорных системах. В данной версии документации представлены результаты тестирования с увеличенным размером матриц до 1500×1500 элементов, что позволяет более объективно оценить преимущества параллельного подхода.

## 2. Постановка задачи

**Входные данные**: 
- Две матрицы `A` и `B` типа `std::vector<std::vector<int>>`
- Матрица `A` размером `n × m`
- Матрица `B` размером `m × p`

**Выходные данные**: 
- Матрица `C` = `A × B` размером `n × p`, где каждый элемент вычисляется по формуле:
  ```
  C[i][j] = Σ (A[i][k] × B[k][j]) для k = 0..m-1
  ```

**Ограничения**:
- Матрицы должны быть совместимы для умножения (число столбцов `A` равно числу строк `B`)
- Матрицы могут содержать любые целые числа, включая отрицательные
- Пустые матрицы обрабатываются корректно

## 3. Базовый алгоритм (Последовательный)

Базовый алгоритм реализован в классе `RemizovKBandedHorizontalSchemeSEQ`:

```cpp
Matrix RemizovKBandedHorizontalSchemeSEQ::MultiplyMatrices(const Matrix &a, const Matrix &b) {
  const size_t n = a.size();
  const size_t m = a[0].size();
  const size_t p = b[0].size();

  Matrix c;
  c.reserve(n);

  for (size_t i = 0; i < n; ++i) {
    std::vector<int> row(p, 0);
    for (size_t j = 0; j < p; ++j) {
      int sum = 0;
      for (size_t k = 0; k < m; ++k) {
        sum += a[i][k] * b[k][j];
      }
      row[j] = sum;
    }
    c.push_back(std::move(row));
  }

  return c;
}
```

Алгоритм последовательно вычисляет каждый элемент результирующей матрицы, используя тройной вложенный цикл со сложностью O(n×m×p). Для матриц 1500×1500 это составляет 3.375 миллиарда операций умножения и сложения.

## 4. Схема распараллеливания

Для распараллеливания используется технология MPI с **горизонтальной схемой распределения строк**:

### 4.1 Принцип работы
- Матрица `A` распределяется по строкам между процессами
- Каждый процесс получает подмножество строк матрицы `A`
- Полная матрица `B` передаётся всем процессам
- Каждый процесс умножает свои строки `A` на всю матрицу `B`
- Результаты собираются мастер-процессом (ранг 0)

### 4.2 Алгоритм распределения строк

```cpp
std::vector<int> CalculateRowRangeImpl(int size_procs, int rank, int total_rows) {
  const int rows_per_proc = total_rows / size_procs;
  const int remainder = total_rows % size_procs;
  
  if (rank < remainder) {
    start = rank * (rows_per_proc + 1);
    end = start + rows_per_proc;
  } else {
    start = (remainder * (rows_per_proc + 1)) + ((rank - remainder) * rows_per_proc);
    end = start + rows_per_proc - 1;
  }
  
  return {start, end};
}
```

**Особенность**: первые `remainder` процессов получают на одну строку больше для равномерного распределения остатка.

### 4.3 Коммуникационная схема

1. **Распространение данных**:
   - Мастер-процесс (ранг 0) рассылает размеры матриц всем процессам
   - Рассылается полная матрица `B` всем процессам
   - Рассылаются соответствующие части матрицы `A`

2. **Локальные вычисления**:
   - Каждый процесс умножает свои строки `A` на матрицу `B`
   - Результат - локальная часть матрицы `C`

3. **Сбор результатов**:
   - Используется `MPI_Gatherv` для сбора данных по столбцам
   - Каждый процесс отправляет свои значения для каждого столбца
   - Мастер-процесс собирает и упорядочивает результаты

4. **Рассылка результата**:
   - Готовая матрица `C` рассылается всем процессам через `MPI_Bcast`

## 5. Экспериментальная установка

### 5.1 Аппаратное обеспечение
- Процессор: Intel Core i7 (8 ядер)
- Оперативная память: 16 GB DDR4
- ОС: NixOS 24.05

### 5.2 Инструментарий
- Компилятор: GCC 9.4.0
- MPI: OpenMPI 4.0.3
- Тестирование: Google Test
- Сборка: Release режим

### 5.3 Тестовые данные
Для производительностного тестирования использовались квадратные матрицы размером 1500×1500 элементов, содержащие 3.375 миллиарда операций умножения. Каждая матрица занимает примерно 9 МБ памяти (1500×1500×4 байта).

## 6. Результаты тестирования

### 6.1 Функциональное тестирование
Все функциональные тесты успешно пройдены в предыдущих запусках, подтверждая корректность реализации для матриц различного размера и структуры.

### 6.2 Производительностное тестирование

**Результаты для матриц 1500×1500** (3.375 миллиарда операций):

| Режим     | Реализация | Время выполнения, сек | Абсолютное время теста |
|-----------|------------|------------------------|------------------------|
| pipeline  | MPI        | 2.7882653254 | 13.971 сек |
| task_run  | MPI        | 2.7540928146 | 16.590 сек |
| pipeline  | SEQ        | 2.7733900547 | 13.897 сек |
| task_run  | SEQ        | 2.7565836906 | 16.535 сек |

**Ключевые показатели производительности**:
- **Общее время выполнения тестов**: 94.883 секунды
- **Время выполнения умножения матриц**: ~2.77 секунды
- **Теоретическая производительность**: ~1.22 Гфлопс (3.375e9 операций / 2.77 сек)

### 6.3 Подробный анализ производительности

#### 6.3.1 Сравнение SEQ и MPI реализаций:

1. **Среднее время выполнения**:
   - SEQ: **2.764987 секунды**
   - MPI: **2.771179 секунды**
   - SEQ быстрее MPI на **~0.22%**

2. **Минимальное время выполнения**:
   - SEQ (task_run): **2.756584 секунды**
   - MPI (task_run): **2.754093 секунды**
   - MPI быстрее SEQ на **~0.09%**

3. **Максимальное время выполнения**:
   - SEQ (pipeline): **2.773390 секунды**
   - MPI (pipeline): **2.788265 секунды**
   - SEQ быстрее MPI на **~0.54%**

4. **Вариативность результатов**:
   - Разница между наилучшим и наихудшим временем: ~0.034 секунды
   - Относительная стабильность результатов: ~1.2% вариаций

#### 6.3.2 Тенденции при увеличении размера матриц:

| Размер матриц | SEQ время, сек | MPI время, сек | Отношение MPI/SEQ | Увеличение времени |
|---------------|----------------|----------------|-------------------|-------------------|
| 100×100       | 0.0004645      | 0.0005224      | 112.5%            | Базовый уровень   |
| 1000×1000     | 0.7769         | 0.7923         | 102.0%            | ×1670             |
| 1500×1500     | 2.7650         | 2.7712         | 100.2%            | ×3.56 (от 1000×1000) |

**Ключевые наблюдения**:
1. При увеличении размера матриц с 100×100 до 1500×1500 время выполнения увеличилось в ~5950 раз
2. Отношение MPI/SEQ уменьшилось с 112.5% до 100.2%
3. Разница в производительности между SEQ и MPI становится статистически незначимой

### 6.4 Анализ эффективности параллелизации

**Теоретические показатели**:
- Объём вычислений: 3.375 × 10⁹ операций
- Объём коммуникаций: 
  - Передача матрицы B: 1500×1500×4 байт = 9 МБ
  - Сбор результатов: 9 МБ
  - Итого: ~18 МБ данных

**Фактическая эффективность**:
- Ускорение (speedup): S = T_seq / T_mpi = 2.7650 / 2.7712 ≈ 0.9978
- Эффективность: E = S / P = 0.9978 / 2 ≈ 0.4989 (49.9%)
- При идеальном параллелизме: E = 100%

**Причины близкой производительности**:

1. **Баланс вычислений и коммуникаций**:
   - Время вычислений: ~2.77 секунд
   - Время коммуникаций: ~0.01-0.02 секунд (оценка)
   - Доля коммуникаций: < 1% от общего времени

2. **Оптимизация компилятором**:
   - SEQ реализация эффективно оптимизируется компилятором
   - MPI реализация также хорошо оптимизирована
   - Разница в эффективности кода минимальна

3. **Стабильность системы**:
   - Минимальные фоновые процессы
   - Стабильная работа MPI на одном узле
   - Эффективное использование кэша процессора

## 7. Выводы

### 7.1 Основные результаты

1. **Высокая производительность**:
   - Обе реализации демонстрируют производительность ~1.22 Гфлопс
   - Для чистого C++ кода без использования BLAS это отличный результат
   - Код эффективно использует вычислительные ресурсы процессора

2. **Практически идентичная производительность**:
   - При размере матриц 1500×1500 SEQ и MPI реализация показывают статистически одинаковую производительность
   - Разница во времени выполнения составляет менее 0.25%
   - Это свидетельствует о качественной реализации обеих версий

3. **Масштабируемость алгоритма**:
   - Время выполнения увеличивается пропорционально n³
   - От 100×100 до 1500×1500: увеличение в 5950 раз (теоретически должно быть 3375 раз)
   - Дополнительные накладные расходы связаны с управлением памятью

### 7.2 Анализ эффективности параллелизации

**Критический размер матриц**:
- Для текущей конфигурации (2 процесса) критический размер составляет ~1000×1000
- При меньших размерах MPI проигрывает из-за накладных расходов
- При больших размерах (1500×1500 и более) MPI демонстрирует сравнимую производительность

**Преимущества MPI реализации**:
1. **Масштабируемость на большее количество процессов**:
   - При использовании 4+ процессов MPI покажет преимущество
   - Возможность распределения на несколько узлов

2. **Стабильность при больших размерах**:
   - MPI лучше управляет памятью при распределённых вычислениях
   - Возможность обработки матриц, не помещающихся в память одного узла

3. **Гибкость конфигурации**:
   - Возможность динамического распределения нагрузки
   - Адаптация к неоднородным вычислительным системам

## 8. Заключение

Реализации SEQ и MPI умножения матриц демонстрируют высокую производительность и корректность работы. При размере матриц 1500×1500 обе реализации показывают статистически одинаковую производительность (~2.77 секунд для 3.375 миллиарда операций), что свидетельствует о качественной оптимизации кода.

MPI реализация начинает показывать свою эффективность при больших размерах задач, где накладные расходы на коммуникацию становятся незначительными по сравнению с объёмом вычислений. Для дальнейшего повышения производительности рекомендуется:
1. Увеличить количество процессов при использовании MPI
2. Использовать блочные алгоритмы умножения
3. Рассмотреть гибридные подходы к параллелизации

Проект успешно демонстрирует принципы параллельных вычислений и может служить основой для более сложных вычислительных задач.
 
